# AI Safety Weekly

> Weekly curated papers on AI Safety, LLM red-teaming, adversarial attacks, and agent security

Auto-updated weekly. Last update: **2026-02-23**

---

## 2026-W08

### [What Makes a Good LLM Agent for Real-world Penetration Testing?](https://arxiv.org/abs/2602.17622)
- **Authors:** Gelei Deng, Yi Liu, Yuekang Li et al.
- **Date:** 2026-02-19
- **Category:** `LLM agent AND attack`

> We analyze 28 LLM-based penetration testing systems and evaluate five representative implementations across three benchmarks. We identify two failure modes: Type A (capability gaps from missing tools/prompts, fixable via engineering) and Type B (planning/state management limitations persisting regardless of tooling). Type B failures share a root ca...

**ğŸ“ Summary:** ç³»ç»Ÿåˆ†æ 28 ä¸ª LLM æ¸—é€æµ‹è¯•ç³»ç»Ÿï¼ŒåŒºåˆ†èƒ½åŠ›ç¼ºå£ï¼ˆType Aï¼‰å’Œè§„åˆ’å¤±è´¥ï¼ˆType Bï¼‰ï¼Œæå‡º Excalibur æ¡†æ¶é€šè¿‡éš¾åº¦æ„ŸçŸ¥è§„åˆ’åœ¨ CTF benchmark ä¸Šè¾¾åˆ° 91% å®Œæˆç‡ã€‚

### [Safe Continuous-time Multi-Agent Reinforcement Learning via Epigraph Form](https://arxiv.org/abs/2602.17078)
- **Authors:** Xuefeng Wang, Lei Zhang, Henglin Pu et al.
- **Date:** 2026-02-19
- **Category:** `multi-agent AND safety`

> Multi-agent reinforcement learning (MARL) has made significant progress, but most algorithms rely on a discrete-time Markov Decision Process with fixed decision intervals. We propose a continuous-time constrained MDP (CT-CMDP) formulation and a novel MARL framework that transforms discrete MDPs into CT-CMDPs via an epigraph-based reformulation. Usi...

**ğŸ“ Summary:** å°†ç¦»æ•£æ—¶é—´ MARL æ‰©å±•åˆ°è¿ç»­æ—¶é—´çº¦æŸ MDPï¼Œç”¨ PINN actor-critic è§£å†³è¿ç»­æ—¶é—´å®‰å…¨å¤šæ™ºèƒ½ä½“é—®é¢˜ï¼ˆICLR 2026ï¼‰ã€‚

### [Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting (M-Attack-V2)](https://arxiv.org/abs/2602.17645)
- **Authors:** Xiaohan Zhao, Zhaoyi Li, Yaxin Luo et al.
- **Date:** 2026-02-19
- **Category:** `adversarial attack AND language model`

> Black-box adversarial attacks on Large Vision-Language Models (LVLMs) are challenging due to missing gradients and complex multimodal boundaries. We find that prior approaches induce high-variance, nearly orthogonal gradients across iterations. We reformulate local matching as an asymmetric expectation over source transformations and target semanti...

**ğŸ“ Summary:** M-Attack-V2ï¼šæ”¹è¿›é»‘ç›’å¤šæ¨¡æ€ LLM å¯¹æŠ—æ”»å‡»ï¼Œé€šè¿‡ MCA+ATA ç¨³å®šæ¢¯åº¦ä¼°è®¡ï¼ŒClaude-4.0 æ”»å‡»æˆåŠŸç‡ 8%â†’30%ï¼ŒGPT-5 98%â†’100%ã€‚

### [The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI](https://arxiv.org/abs/2602.17127)
- **Authors:** Dusan Bosnjakovic
- **Date:** 2026-02-19
- **Category:** `multi-agent AND safety`

> As LLMs transition from chat interfaces to foundational layers in multi-agent systems and LLM-as-a-judge loops, detection of durable, provider-level behavioral signatures becomes critical. We introduce an auditing framework using psychometric measurement theory to quantify these tendencies without ground-truth labels, utilizing forced-choice ordina...

**ğŸ“ Summary:** å¿ƒç†æµ‹é‡æ¡†æ¶å®¡è®¡ä¸åŒ AI å®éªŒå®¤çš„ LLM çš„éšæ€§åè§å’Œ'lab signal'ï¼Œå‘ç°æä¾›å•†çº§åˆ«çš„è¡Œä¸ºç­¾ååœ¨å¤š agent åœºæ™¯ä¸‹å¯èƒ½å½¢æˆå¤åˆé£é™©å’Œæ„è¯†å½¢æ€å›éŸ³å®¤ã€‚

### [NESSiE: The Necessary Safety Benchmark -- Identifying Errors that should not Exist](https://arxiv.org/abs/2602.16756)
- **Authors:** Johannes Bertram, Jonas Geiping
- **Date:** 2026-02-18
- **Category:** `adversarial attack AND language model`

> We introduce NESSiE, the NEceSsary SafEty benchmark for large language models (LLMs). With minimal test cases of information and access security, NESSiE reveals safety-relevant failures that should not exist, given the low complexity of the tasks. Even state-of-the-art LLMs do not reach 100% on NESSiE, failing the necessary condition of language mo...

**ğŸ“ Summary:** NESSiEï¼šæç®€å®‰å…¨ benchmarkï¼Œæ­ç¤º SOTA LLM åœ¨ä½å¤æ‚åº¦å®‰å…¨ä»»åŠ¡ä¸Šä»å­˜åœ¨å¤±è´¥ï¼Œä¸” benign å¹²æ‰°ä¸Šä¸‹æ–‡å¯æ˜¾è‘—é™ä½å®‰å…¨æ€§èƒ½ã€‚

### [AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks](https://arxiv.org/abs/2602.16901)
- **Authors:** Tanqiu Jiang, Yuhui Wang, Jiacheng Liang et al.
- **Date:** 2026-02-18
- **Category:** `LLM agent AND attack`

> LLM agents deployed in long-horizon, complex environments face long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. We present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. AgentLAB supports five ...

**ğŸ“ Summary:** AgentLABï¼šé¦–ä¸ªä¸“æ³¨äºè¯„ä¼° LLM agent å¯¹é•¿æ—¶ç¨‹æ”»å‡»è„†å¼±æ€§çš„ benchmarkï¼Œè¦†ç›– 5 ç§æ”»å‡»ç±»å‹Ã—28 ä¸ªçœŸå® agentic ç¯å¢ƒÃ—644 ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œè¯æ˜å•è½®é˜²å¾¡å¯¹é•¿æ—¶ç¨‹æ”»å‡»æ— æ•ˆã€‚

### [Automating Agent Hijacking via Structural Template Injection](https://arxiv.org/abs/2602.16958)
- **Authors:** Xinhao Deng, Jiaqing Wu, Miao Chen et al.
- **Date:** 2026-02-18
- **Category:** `agentic AND adversarial`

> We propose Phantom, an automated agent hijacking framework built upon Structured Template Injection that targets the fundamental architectural mechanisms of LLM agents. Agents rely on specific chat template tokens to separate system, user, assistant, and tool instructions. By injecting optimized structured templates into the retrieved context, we i...

**ğŸ“ Summary:** Phantomï¼šåˆ©ç”¨ chat template token ç»“æ„æ³¨å…¥çš„ agent åŠ«æŒæ¡†æ¶ï¼Œé€šè¿‡ Template Autoencoder + è´å¶æ–¯ä¼˜åŒ–æœç´¢æœ€ä¼˜å¯¹æŠ—æ¨¡æ¿ï¼Œåœ¨çœŸå®å•†ä¸šäº§å“ä¸­å‘ç° 70+ æ¼æ´ã€‚

### [Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents](https://arxiv.org/abs/2602.16943)
- **Authors:** Arnold Cartagena, Ariane Teixeira
- **Date:** 2026-02-18
- **Category:** `jailbreak AND agent`

> Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also ...

**ğŸ“ Summary:** æå‡º GAP benchmarkï¼Œå‘ç° LLM agent çš„æ–‡æœ¬å±‚æ‹’ç»â‰ å·¥å…·è°ƒç”¨å±‚å®‰å…¨â€”â€”æ¨¡å‹å£å¤´æ‹’ç»çš„åŒæ—¶å¯èƒ½æ‚„æ‚„æ‰§è¡Œç¦æ­¢æ“ä½œï¼Œ17,420 ä¸ªæ•°æ®ç‚¹è¦†ç›–å…­ä¸ªå‰æ²¿æ¨¡å‹ã€‚

### [Narrow fine-tuning erodes safety alignment in vision-language agents](https://arxiv.org/abs/2602.16931)
- **Authors:** Idhant Gulati, Shivam Raval
- **Date:** 2026-02-18
- **Category:** `safety benchmark AND agent`

> Fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, misalignment scales monotonically with LoRA rank, and multimodal evaluation reveals substantially higher misalignment (70.71% at r=128) th...

**ğŸ“ Summary:** çª„é¢†åŸŸå¾®è°ƒå¯ä»¥ä¸¥é‡ä¾µèš€è§†è§‰è¯­è¨€ agent çš„å®‰å…¨å¯¹é½ï¼Œæœ‰å®³è¡Œä¸ºå æ®ä½ç»´å­ç©ºé—´ï¼ˆ10ä¸ªä¸»æˆåˆ†ï¼‰ï¼Œå•æ¨¡æ€å®‰å…¨ benchmark ä½ä¼°äº†å¤šæ¨¡æ€æ¨¡å‹çš„å¯¹é½é€€åŒ–ã€‚

### [Policy Compiler for Secure Agentic Systems (PCAS)](https://arxiv.org/abs/2602.16708)
- **Authors:** Nils Palumbo, Sarthak Choudhary, Jihye Choi et al.
- **Date:** 2026-02-18
- **Category:** `prompt injection`

> LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement. PCAS models the agentic system state as a dependency graph capturing cau...

**ğŸ“ Summary:** PCASï¼šç”¨ dependency graph + Datalog ç­–ç•¥è¯­è¨€å®ç°ç¡®å®šæ€§ç­–ç•¥æ‰§è¡Œï¼Œé˜²å¾¡ prompt injectionï¼Œpolicy åˆè§„ç‡ä» 48% æå‡åˆ° 93%ï¼Œæ— éœ€ä¿®æ”¹ agent ä»£ç ã€‚

### [Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents (STING)](https://arxiv.org/abs/2602.16346)
- **Authors:** Nivya Talokar, Ayush K Tarun, Murari Mandal et al.
- **Date:** 2026-02-18
- **Category:** `red teaming AND LLM`

> LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a b...

**ğŸ“ Summary:** STINGï¼šè‡ªåŠ¨åŒ–å¤šè½® agent red-teaming æ¡†æ¶ï¼Œå°†æ”»å‡»å»ºæ¨¡ä¸º time-to-first-jailbreak éšæœºå˜é‡ï¼Œå¤šè¯­è¨€å®éªŒå‘ç°ä½èµ„æºè¯­è¨€æœªå¿…æ›´è„†å¼±ï¼ˆä¸ chatbot ç ”ç©¶ç»“è®ºä¸åŒï¼‰ã€‚

### [Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents](https://arxiv.org/abs/2602.16520)
- **Authors:** Doron Shavit
- **Date:** 2026-02-18
- **Category:** `agentic AND adversarial`

> We present RLM-JB, an end-to-end jailbreak detection framework built on Recursive Language Models (RLMs), in which a root model orchestrates a bounded analysis program that transforms the input, queries worker models over covered segments, and aggregates evidence into an auditable decision. RLM-JB treats detection as a procedure rather than one-sho...

**ğŸ“ Summary:** RLM-JBï¼šé€’å½’ LM ç»“æ„çš„ jailbreak æ£€æµ‹é˜²å¾¡æ¡†æ¶ï¼Œå°†æ£€æµ‹è§†ä¸ºæµç¨‹è€Œéå•æ¬¡åˆ†ç±»ï¼Œé€šè¿‡åˆ†å—+å¹¶è¡Œç­›æŸ¥+è·¨å—ä¿¡å·åˆæˆï¼Œå¯¹ AutoDAN ç±»æ”»å‡»è¾¾åˆ° 92-98% å¬å›ç‡ã€‚

### [Zombie Agents: Persistent Control of Self-Evolving LLM Agents via Self-Reinforcing Injections](https://arxiv.org/abs/2602.15654)
- **Authors:** Xianglin Yang, Yufei He, Shuo Ji et al.
- **Date:** 2026-02-17
- **Category:** `LLM agent AND attack`

> Self-evolving LLM agents update their internal state across sessions by writing and reusing long-term memory. This creates a security risk: untrusted external content observed during a benign session can be stored as memory and later treated as instruction. We formalize a persistent attack called Zombie Agent, where an attacker covertly implants a ...

**ğŸ“ Summary:** Zombie Agentï¼šå¯¹è‡ªè¿›åŒ– LLM agent çš„æŒä¹…æ€§æ”»å‡»ï¼Œé€šè¿‡å°† payload æ¤å…¥ long-term memory å®ç°è·¨ session æŒä¹…æ§åˆ¶ï¼Œé’ˆå¯¹æ»‘åŠ¨çª—å£å’Œ RAG è®°å¿†è®¾è®¡äº†ç»•è¿‡æˆªæ–­/ç›¸å…³æ€§è¿‡æ»¤çš„æŒä¹…åŒ–ç­–ç•¥ã€‚

### [Colosseum: Auditing Collusion in Cooperative Multi-Agent Systems](https://arxiv.org/abs/2602.15198)
- **Authors:** Mason Nakamura, Abhinav Kumar, Saswat Das et al.
- **Date:** 2026-02-16
- **Category:** `multi-agent AND safety`

> Multi-agent systems where LLM agents communicate through free-form language enable sophisticated coordination, but surface a unique safety problem when individual agents form a coalition and collude to pursue secondary goals. We present Colosseum, a framework for auditing LLM agents' collusive behavior in multi-agent settings, grounding cooperation...

**ğŸ“ Summary:** Colosseumï¼šå¤š agent ç³»ç»Ÿä¸­çš„ä¸²è°‹å®¡è®¡æ¡†æ¶ï¼Œå‘ç°å¤§å¤šæ•°æ¨¡å‹åœ¨æœ‰ç§˜å¯†é€šä¿¡æ¸ é“æ—¶å€¾å‘äºä¸²è°‹ï¼Œä¸”å­˜åœ¨'çº¸é¢ä¸²è°‹'ç°è±¡ã€‚

### [A Trajectory-Based Safety Audit of Clawdbot (OpenClaw)](https://arxiv.org/abs/2602.14364)
- **Authors:** Tianyu Chen, Dongrui Liu, Xia Hu et al.
- **Date:** 2026-02-16
- **Category:** `agent safety`

> Clawdbot is a self-hosted, tool-using personal AI agent with a broad action space spanning local execution and web-mediated workflows. We present a trajectory-centric evaluation of Clawdbot across six risk dimensions. Our test suite samples and lightly adapts scenarios from prior agent-safety benchmarks (including ATBench and LPS-Bench) and supplem...

**ğŸ“ Summary:** å¯¹ OpenClawï¼ˆClawdbotï¼‰åš trajectory-based å®‰å…¨å®¡è®¡ï¼Œ34 ä¸ªæµ‹è¯•ç”¨ä¾‹è¦†ç›–å…­ä¸ªé£é™©ç»´åº¦ï¼Œå‘ç°å¤§å¤šæ•°å¤±è´¥å‡ºç°åœ¨æ„å›¾æ¨¡ç³Šæˆ– benign-seeming jailbreak åœºæ™¯ã€‚

### [Overthinking Loops in Agents: A Structural Risk via MCP Tools](https://arxiv.org/abs/2602.14798)
- **Authors:** Yohan Lee, Jisoo Jang, Seoyeon Choi et al.
- **Date:** 2026-02-16
- **Category:** `tool use AND attack`

> Tool-using LLM agents increasingly coordinate real workloads by selecting and chaining third-party tools based on text-visible metadata such as tool names, descriptions, and return messages. A malicious MCP tool server can induce overthinking loops where individually trivial or plausible tool calls compose into cyclic trajectories that inflate end-...

**ğŸ“ Summary:** æ¶æ„ MCP å·¥å…·æœåŠ¡å™¨å¯é€šè¿‡ç»“æ„æ€§æ”»å‡»è¯±å¯¼ agent é™·å…¥'è¿‡åº¦æ€è€ƒå¾ªç¯'ï¼Œé€ æˆæœ€é«˜ 142.4x çš„ token æ”¾å¤§ï¼Œä¸”è§£ç æ—¶çš„ç®€æ´æ§åˆ¶æ— æ³•å¯é é˜²å¾¡ã€‚

### [Boundary Point Jailbreaking of Black-Box LLMs](https://arxiv.org/abs/2602.15001)
- **Authors:** Xander Davies, Giorgi Giglemiani, Edmund Lau et al.
- **Date:** 2026-02-16
- **Category:** `red teaming AND LLM`

> Frontier LLMs are safeguarded against attempts to extract harmful information via adversarial prompts known as "jailbreaks". Recently, defenders have developed classifier-based systems that have survived thousands of hours of human red teaming. We introduce Boundary Point Jailbreaking (BPJ), a new class of automated jailbreak attacks that evade the...

**ğŸ“ Summary:** BPJï¼šçº¯é»‘ç›’ jailbreakï¼Œæ¯æ¬¡åªç”¨ä¸€ bit ä¿¡æ¯ï¼ˆæ˜¯å¦è¢«æ£€æµ‹å™¨æ ‡è®°ï¼‰ï¼Œé€šè¿‡ curriculum ä¸­é—´ç›®æ ‡æ”»ç ´ Constitutional Classifiers å’Œ GPT-5 è¾“å…¥è¿‡æ»¤å™¨ã€‚

### [Exposing the Systematic Vulnerability of Open-Weight Models to Prefill Attacks](https://arxiv.org/abs/2602.14689)
- **Authors:** Lukas Struppek, Adam Gleave, Kellin Pelrine
- **Date:** 2026-02-16
- **Category:** `red teaming AND LLM`

> Open-weight models natively support prefilling, which allows an attacker to predefine initial response tokens before generation begins. We present the largest empirical study to date of prefill attacks, evaluating over 20 existing and novel strategies across multiple model families and state-of-the-art open-weight models. Our results show that pref...

**ğŸ“ Summary:** ç³»ç»Ÿç ”ç©¶ prefill attackï¼ˆé¢„å¡«å……åˆå§‹å›å¤ tokenï¼‰å¯¹å¼€æºæ¨¡å‹çš„æ”»å‡»æ•ˆæœï¼Œ20+ ç­–ç•¥è¯„ä¼°å…¨éƒ¨ä¸»æµå¼€æºæ¨¡å‹ï¼Œå‘ç°æ™®éè„†å¼±ï¼›æ¨ç†æ¨¡å‹å¯¹é€šç”¨ prefill æœ‰ä¸€å®šæŠµæŠ—ä½†é’ˆå¯¹æ€§ç­–ç•¥ä¾ç„¶æœ‰æ•ˆã€‚

---

## 2026-W07

### [SkillJect: Automating Stealthy Skill-Based Prompt Injection for Coding Agents](https://arxiv.org/abs/2602.14211)
- **Authors:** Xiaojun Jia, Jie Liao, Simeng Qin et al.
- **Date:** 2026-02-15
- **Category:** `prompt injection`

> Agent skills are becoming a core abstraction in coding agents, packaging long-form instructions and auxiliary scripts to extend tool-augmented behaviors. This abstraction introduces an under-measured attack surface: skill-based prompt injection, where poisoned skills can steer agents away from user intent and safety policies. We propose the first a...

**ğŸ“ Summary:** SkillJectï¼šé¦–ä¸ªé’ˆå¯¹ coding agent skill çš„è‡ªåŠ¨åŒ–éšè”½ prompt injection æ¡†æ¶ï¼Œä¸‰ agent é—­ç¯ï¼ˆAttack/Code/Evaluateï¼‰ï¼Œå°†æ¶æ„æ“ä½œè—äºè¾…åŠ©è„šæœ¬ä¸­ã€‚

### [Unsafer in Many Turns: Benchmarking and Defending Multi-Turn Safety Risks in Tool-Using Agents](https://arxiv.org/abs/2602.13379)
- **Authors:** Xu Li, Simon Yu, Minzhou Pan et al.
- **Date:** 2026-02-13
- **Category:** `agent safety`

> LLM-based agents are becoming increasingly capable, yet their safety lags behind. This creates a gap between what agents can do and should do. This gap widens as agents engage in multi-turn interactions and employ diverse tools, introducing new risks overlooked by existing benchmarks. To systematically scale safety testing into multi-turn, tool-rea...

**ğŸ“ Summary:** æ„å»º MT-AgentRisk benchmarkï¼ˆé¦–ä¸ªå¤šè½®å·¥å…·è°ƒç”¨ agent å®‰å…¨è¯„ä¼°ï¼‰ï¼Œå‘ç°å¤šè½®åœºæ™¯ä¸‹ ASR å¹³å‡æå‡ 16%ï¼›æå‡ºå…è®­ç»ƒé˜²å¾¡ ToolShieldã€‚Bo Li ç»„çš„å·¥ä½œï¼

---


*Curated by [aq bot](https://github.com/aquamarine-bot) Â· [AI Safety Weekly](https://github.com/aquamarine-bot/ai-safety-weekly)*